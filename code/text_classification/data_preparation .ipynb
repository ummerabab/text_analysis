{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading english stopwords defined in nltk module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00-1001.txt</td>\n",
       "      <td>Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00-1002.txt</td>\n",
       "      <td>Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00-1004.txt</td>\n",
       "      <td>Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00-1005.txt</td>\n",
       "      <td>Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00-1006.txt</td>\n",
       "      <td>Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>J96-3002.txt</td>\n",
       "      <td>Grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>M93-1003.txt</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>M93-1004.txt</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>M93-1006.txt</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>M93-1009.txt</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             file        label\n",
       "0    A00-1001.txt       Speech\n",
       "1    A00-1002.txt  Translation\n",
       "2    A00-1004.txt  Translation\n",
       "3    A00-1005.txt       Speech\n",
       "4    A00-1006.txt  Translation\n",
       "..            ...          ...\n",
       "438  J96-3002.txt      Grammar\n",
       "439  M93-1003.txt           IE\n",
       "440  M93-1004.txt           IE\n",
       "441  M93-1006.txt           IE\n",
       "442  M93-1009.txt           IE\n",
       "\n",
       "[443 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Give path of new_labels.txt with /\n",
    "pd.read_csv(r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\new_labels.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file names and labels\n",
    "\n",
    "file_labels = pd.read_csv(r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\new_labels.txt')\n",
    "file_labels.to_csv (r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\read_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = file_labels.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Speech           121\n",
       "Translation       93\n",
       "IE                67\n",
       "Grammar           55\n",
       "Lexical           35\n",
       "QA                27\n",
       "Summarization     23\n",
       "NE                22\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vieing label distribution\n",
    "file_labels.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A00-1001.txt\n",
      "A00-1002.txt\n",
      "A00-1004.txt\n",
      "A00-1005.txt\n",
      "A00-1006.txt\n",
      "A00-1007.txt\n",
      "A00-1009.txt\n",
      "A00-1010.txt\n",
      "A00-1011.txt\n",
      "A00-1012.txt\n",
      "A00-1013.txt\n",
      "A00-1015.txt\n",
      "A00-1014.txt\n",
      "A00-1016.txt\n",
      "A00-1018.txt\n",
      "A00-1021.txt\n",
      "A00-1023.txt\n",
      "A00-1023.txt\n",
      "A00-1025.txt\n",
      "A00-1027.txt\n",
      "A00-1028.txt\n",
      "A00-1029.txt\n",
      "A00-1032.txt\n",
      "A00-1034.txt\n",
      "A00-1035.txt\n",
      "A00-1036.txt\n",
      "A00-1038.txt\n",
      "A00-1039.txt\n",
      "A00-1040.txt\n",
      "A00-1041.txt\n",
      "A00-1042.txt\n",
      "A00-1043.txt\n",
      "A00-1044.txt\n",
      "A00-1045.txt\n",
      "A00-2001.txt\n",
      "A00-2011.txt\n",
      "A00-2013.txt\n",
      "A00-2014.txt\n",
      "A00-2019.txt\n",
      "A00-2021.txt\n",
      "A00-2022.txt\n",
      "A00-2024.txt\n",
      "A00-2025.txt\n",
      "A00-2025.txt\n",
      "A00-2027.txt\n",
      "A00-2028.txt\n",
      "A00-2029.txt\n",
      "A00-2030.txt\n",
      "A00-2033.txt\n",
      "A00-2035.txt\n",
      "A00-2036.txt\n",
      "A00-2037.txt\n",
      "A00-2039.txt\n",
      "A00-3004.txt\n",
      "A00-3007.txt\n",
      "A83-1005.txt\n",
      "A83-1009.txt\n",
      "A83-1017.txt\n",
      "A83-1019.txt\n",
      "A88-1010.txt\n",
      "A88-1014.txt\n",
      "A88-1016.txt\n",
      "A88-1018.txt\n",
      "A88-1020.txt\n",
      "A94-1005.txt\n",
      "A94-1006.txt\n",
      "A94-1007.txt\n",
      "A94-1008.txt\n",
      "A94-1009.txt\n",
      "A94-1015.txt\n",
      "A94-1016.txt\n",
      "A94-1017.txt\n",
      "A94-1019.txt\n",
      "A94-1044.txt\n",
      "A94-1047.txt\n",
      "A97-1001.txt\n",
      "A97-1003.txt\n",
      "A97-2012.txt\n",
      "A97-2020.txt\n",
      "C00-1002.txt\n",
      "C00-1005.txt\n",
      "C00-1006.txt\n",
      "C00-1008.txt\n",
      "C00-1009.txt\n",
      "C00-1011.txt\n",
      "C00-1018.txt\n",
      "C00-1019.txt\n",
      "C00-1025.txt\n",
      "C00-1028.txt\n",
      "C00-1030.txt\n",
      "C00-1034.txt\n",
      "C00-1039.txt\n",
      "C00-1040.txt\n",
      "C00-1043.txt\n",
      "C00-1044.txt\n",
      "C00-1046.txt\n",
      "C00-1048.txt\n",
      "C00-1052.txt\n",
      "C00-1055.txt\n",
      "C00-1056.txt\n",
      "C00-1059.txt\n",
      "C00-1061.txt\n",
      "C00-1062.txt\n",
      "C00-1064.txt\n",
      "C00-1066.txt\n",
      "C00-1068.txt\n",
      "C00-1070.txt\n",
      "C00-1072.txt\n",
      "C00-1073.txt\n",
      "C00-1074.txt\n",
      "C00-1075.txt\n",
      "C00-1078.txt\n",
      "C00-1079.txt\n",
      "C00-1084.txt\n",
      "C00-2087.txt\n",
      "C00-2089.txt\n",
      "C00-2090.txt\n",
      "C00-2092.txt\n",
      "C00-2094.txt\n",
      "C00-2098.txt\n",
      "C00-2100.txt\n",
      "C00-2101.txt\n",
      "C00-2102.txt\n",
      "C00-2103.txt\n",
      "C00-2104.txt\n",
      "C00-2105.txt\n",
      "C00-2106.txt\n",
      "C00-2107.txt\n",
      "C00-2113.txt\n",
      "C00-2122.txt\n",
      "C00-2123.txt\n",
      "C00-2125.txt\n",
      "C00-2127.txt\n",
      "C00-2129.txt\n",
      "C00-2134.txt\n",
      "C00-2135.txt\n",
      "C00-2136.txt\n",
      "C00-2138.txt\n",
      "C00-2139.txt\n",
      "C00-2140.txt\n",
      "C00-2142.txt\n",
      "C00-2143.txt\n",
      "C00-2144.txt\n",
      "C00-2145.txt\n",
      "C00-2147.txt\n",
      "C00-2152.txt\n",
      "C00-2153.txt\n",
      "C00-2154.txt\n",
      "C00-2154.txt\n",
      "C00-2159.txt\n",
      "C00-2160.txt\n",
      "C00-2162.txt\n",
      "C00-2163.txt\n",
      "C00-2165.txt\n",
      "C00-2167.txt\n",
      "C00-2169.txt\n",
      "C00-2172.txt\n",
      "C00-2173.txt\n",
      "C00-2174.txt\n",
      "C02-1016.txt\n",
      "C02-1018.txt\n",
      "C02-1025.txt\n",
      "C02-1026.txt\n",
      "C02-1042.txt\n",
      "C02-1044.txt\n",
      "C02-1045.txt\n",
      "C02-1048.txt\n",
      "C02-1050.txt\n",
      "C02-1053.txt\n",
      "C02-1054.txt\n",
      "C02-1056.txt\n",
      "C02-1057.txt\n",
      "C02-1059.txt\n",
      "C02-1076.txt\n",
      "C02-1080.txt\n",
      "C02-1081.txt\n",
      "C02-1084.txt\n",
      "C02-1085.txt\n",
      "C02-1086.txt\n",
      "C02-1088.txt\n",
      "C02-1095.txt\n",
      "C02-1099.txt\n",
      "C02-1103.txt\n",
      "C02-1119.txt\n",
      "C02-1120.txt\n",
      "C02-1124.txt\n",
      "C02-1129.txt\n",
      "C02-1130.txt\n",
      "C02-1133.txt\n",
      "C02-1134.txt\n",
      "C02-1136.txt\n",
      "C02-1137.txt\n",
      "C02-1150.txt\n",
      "C02-1153.txt\n",
      "C65-1015.txt\n",
      "C65-1016.txt\n",
      "C65-1017.txt\n",
      "C65-1019.txt\n",
      "C65-1020.txt\n",
      "C67-1002.txt\n",
      "C80-1078.txt\n",
      "C82-1012.txt\n",
      "C82-1038.txt\n",
      "C82-1043.txt\n",
      "C82-1044.txt\n",
      "C82-1045.txt\n",
      "C82-1046.txt\n",
      "C82-1057.txt\n",
      "C82-1058.txt\n",
      "C82-1066.txt\n",
      "C82-2020.txt\n",
      "C82-2026.txt\n",
      "C82-2031.txt\n",
      "C82-2032.txt\n",
      "C82-2043.txt\n",
      "C92-2089.txt\n",
      "C92-2090.txt\n",
      "C92-2092.txt\n",
      "C92-2094.txt\n",
      "C92-2097.txt\n",
      "C92-2099.txt\n",
      "C92-2101.txt\n",
      "C92-2120.txt\n",
      "C92-2119.txt\n",
      "C94-1001.txt\n",
      "C94-1003.txt\n",
      "C94-1005.txt\n",
      "C94-1013.txt\n",
      "C94-1014.txt\n",
      "C94-1015.txt\n",
      "C94-1019.txt\n",
      "C94-1021.txt\n",
      "C94-1024.txt\n",
      "C94-1028.txt\n",
      "C94-1032.txt\n",
      "C94-1036.txt\n",
      "C94-1047.txt\n",
      "C94-1065.txt\n",
      "C94-2189.txt\n",
      "C94-2194.txt\n",
      "C94-2196.txt\n",
      "C94-2197.txt\n",
      "C96-1002.txt\n",
      "C90-2006.txt\n",
      "C90-2007.txt\n",
      "C90-2008.txt\n",
      "C90-2010.txt\n",
      "C90-2011.txt\n",
      "C90-2013.txt\n",
      "C90-2016.txt\n",
      "C90-2019.txt\n",
      "C90-2023.txt\n",
      "C90-2024.txt\n",
      "C90-2031.txt\n",
      "C90-2033.txt\n",
      "C90-2034.txt\n",
      "C90-2037.txt\n",
      "C96-2130.txt\n",
      "C96-2133.txt\n",
      "E06-1037.txt\n",
      "E06-1039.txt\n",
      "E06-1043.txt\n",
      "E06-1045.txt\n",
      "E06-1050.txt\n",
      "E06-1051.txt\n",
      "E06-2002.txt\n",
      "E06-2004.txt\n",
      "E06-2009.txt\n",
      "E06-2011.txt\n",
      "E06-2017.txt\n",
      "E06-2020.txt\n",
      "E06-2021.txt\n",
      "E06-2029.txt\n",
      "E06-3005.txt\n",
      "E06-3006.txt\n",
      "E83-1004.txt\n",
      "E83-1005.txt\n",
      "E83-1014.txt\n",
      "E83-1019.txt\n",
      "E83-1027.txt\n",
      "E83-1031.txt\n",
      "E83-1032.txt\n",
      "E85-1005.txt\n",
      "E85-1006.txt\n",
      "E85-1007.txt\n",
      "E85-1008.txt\n",
      "E85-1009.txt\n",
      "E85-1010.txt\n",
      "E85-1017.txt\n",
      "E85-1024.txt\n",
      "E85-1035.txt\n",
      "E85-1040.txt\n",
      "E85-1041.txt\n",
      "E87-1012.txt\n",
      "E87-1014.txt\n",
      "E87-1014.txt\n",
      "E87-1015.txt\n",
      "E87-1017.txt\n",
      "E87-1018.txt\n",
      "E87-1019.txt\n",
      "E87-1020.txt\n",
      "E87-1021.txt\n",
      "E87-1027.txt\n",
      "E87-1029.txt\n",
      "E87-1031.txt\n",
      "E87-1032.txt\n",
      "E87-1034.txt\n",
      "E87-1037.txt\n",
      "E89-1000.txt\n",
      "E89-1001.txt\n",
      "E89-1006.txt\n",
      "E89-1008.txt\n",
      "E93-1003.txt\n",
      "E93-1013.txt\n",
      "E93-1015.txt\n",
      "E95-1027.txt\n",
      "H05-2009.txt\n",
      "H05-2010.txt\n",
      "H89-2063.txt\n",
      "H89-2064.txt\n",
      "H89-2066.txt\n",
      "H89-2067.txt\n",
      "H89-2069.txt\n",
      "H89-2070.txt\n",
      "H89-2071.txt\n",
      "H89-2072.txt\n",
      "H89-2074.txt\n",
      "H89-2076.txt\n",
      "H89-2077.txt\n",
      "H90-1001.txt\n",
      "H90-1002.txt\n",
      "H90-1003.txt\n",
      "H90-1004.txt\n",
      "H90-1005.txt\n",
      "H90-1005.txt\n",
      "H90-1006.txt\n",
      "H90-1008.txt\n",
      "H90-1010.txt\n",
      "H90-1011.txt\n",
      "H90-1016.txt\n",
      "H90-1017.txt\n",
      "H90-1018.txt\n",
      "H90-1021.txt\n",
      "H90-1022.txt\n",
      "H90-1023.txt\n",
      "H90-1024.txt\n",
      "H90-1026.txt\n",
      "H90-1027.txt\n",
      "H90-1029.txt\n",
      "H90-1030.txt\n",
      "H90-1031.txt\n",
      "H90-1032.txt\n",
      "H90-1033.txt\n",
      "H90-1034.txt\n",
      "H90-1036.txt\n",
      "H90-1038.txt\n",
      "H90-1040.txt\n",
      "H90-1041.txt\n",
      "H90-1043.txt\n",
      "H90-1044.txt\n",
      "H90-1045.txt\n",
      "H90-1046.txt\n",
      "H90-1047.txt\n",
      "H90-1048.txt\n",
      "H90-1049.txt\n",
      "H90-1054.txt\n",
      "H90-1055.txt\n",
      "H91-1024.txt\n",
      "H91-1025.txt\n",
      "H91-1026.txt\n",
      "H91-1027.txt\n",
      "H91-1028.txt\n",
      "H91-1029.txt\n",
      "H91-1068.txt\n",
      "H91-1069.txt\n",
      "H91-1070.txt\n",
      "H91-1071.txt\n",
      "H91-1072.txt\n",
      "H91-1073.txt\n",
      "H91-1074.txt\n",
      "H91-1075.txt\n",
      "H91-1078.txt\n",
      "H91-1081.txt\n",
      "H91-1082.txt\n",
      "H91-1083.txt\n",
      "H91-1084.txt\n",
      "H91-1085.txt\n",
      "H91-1086.txt\n",
      "H91-1088.txt\n",
      "H91-1089.txt\n",
      "H91-1090.txt\n",
      "H91-1091.txt\n",
      "H91-1096.txt\n",
      "H91-1098.txt\n",
      "H91-1099.txt\n",
      "H91-1101.txt\n",
      "H92-1065.txt\n",
      "H92-1066.txt\n",
      "H92-1068.txt\n",
      "X98-1031.txt\n",
      "X98-1030.txt\n",
      "X98-1028.txt\n",
      "X98-1027.txt\n",
      "X98-1026.txt\n",
      "X98-1025.txt\n",
      "X98-1022.txt\n",
      "X98-1021.txt\n",
      "X98-1019.txt\n",
      "X98-1018.txt\n",
      "X98-1017.txt\n",
      "X98-1016.txt\n",
      "X98-1015.txt\n",
      "X98-1013.txt\n",
      "X98-1012.txt\n",
      "X98-1008.txt\n",
      "X98-1007.txt\n",
      "X98-1006.txt\n",
      "X98-1005.txt\n",
      "X96-1059.txt\n",
      "X96-1058.txt\n",
      "X96-1052.txt\n",
      "X96-1051.txt\n",
      "X96-1043.txt\n",
      "X96-1042.txt\n",
      "X96-1040.txt\n",
      "X96-1039.txt\n",
      "X96-1037.txt\n",
      "X96-1036.txt\n",
      "X96-1035.txt\n",
      "X96-1014.txt\n",
      "X96-1009.txt\n",
      "J94-3010.txt\n",
      "J94-4002.txt\n",
      "J81-4003.txt\n",
      "J82-1002.txt\n",
      "J82-1003.txt\n",
      "J87-3009.txt\n",
      "J88-3006.txt\n",
      "J96-3002.txt\n",
      "M93-1003.txt\n",
      "M93-1004.txt\n",
      "M93-1006.txt\n",
      "M93-1009.txt\n",
      "11411\n",
      "[('system', 1043), ('language', 581), ('information', 554), ('systems', 504), ('text', 422), ('speech', 402), ('used', 382), ('data', 356), ('translation', 355), ('paper', 346), ('using', 343), ('one', 337), ('two', 315), ('set', 306), ('grammar', 298), ('results', 297), ('word', 295), ('performance', 294), ('document', 292), ('use', 287), ('also', 280), ('recognition', 259), ('query', 257), ('evaluation', 256), ('rules', 253), ('method', 250), ('sentences', 248), ('task', 241), ('sentence', 236), ('natural', 235), ('test', 229), ('words', 228), ('analysts', 226), ('model', 216), ('new', 212), ('based', 211), ('user', 210), ('english', 209), ('may', 205), ('number', 203), ('first', 201), ('different', 191), ('machine', 191), ('approach', 190), ('syntactic', 189), ('analysis', 189), ('lexical', 189), ('knowledge', 188), ('semantic', 187), ('time', 186), ('queries', 179), ('error', 178), ('structure', 178), ('would', 172), ('processing', 172), ('figure', 170), ('extraction', 169), ('parsing', 168), ('input', 166), ('documents', 165), ('terms', 163), ('retrieval', 162), ('template', 161), ('corpus', 160), ('given', 160), ('algorithm', 160), ('possible', 159), ('training', 159), ('spoken', 157), ('domain', 154), ('research', 153), ('work', 149), ('developed', 149), ('grammars', 145), ('process', 144), ('human', 139), ('however', 139), ('example', 139), ('architecture', 137), ('correct', 136), ('type', 132), ('level', 130), ('tipster', 130), ('application', 129), ('object', 129), ('constraints', 128), ('well', 128), ('context', 128), ('methods', 127), ('present', 127), ('answer', 126), ('response', 126), ('string', 125), ('order', 124), ('problem', 123), ('form', 123), ('scoring', 123), ('large', 121), ('development', 121), ('slots', 121), ('since', 119), ('languages', 119), ('search', 119), ('scores', 118), ('general', 117), ('made', 117), ('types', 116), ('features', 116), ('key', 115), ('linguistic', 115), ('provide', 113), ('representation', 112), ('parser', 112), ('many', 111), ('techniques', 111), ('accuracy', 111), ('particular', 111), ('texts', 107), ('class', 107), ('models', 105), ('analyst', 104), ('part', 103), ('must', 103), ('described', 102), ('users', 101), ('three', 100), ('several', 100), ('best', 99), ('dialogue', 98), ('output', 98), ('templates', 98), ('fill', 98), ('structures', 97), ('function', 97), ('precision', 97), ('show', 96), ('applications', 96), ('automatic', 96), ('collection', 96), ('even', 94), ('understanding', 94), ('al', 93), ('database', 92), ('coreference', 92), ('table', 91), ('japanese', 91), ('mt', 90), ('way', 90), ('case', 90), ('could', 89), ('question', 88), ('simple', 88), ('ie', 87), ('within', 87), ('generation', 86), ('shows', 85), ('single', 85), ('following', 85), ('reference', 84), ('high', 84), ('implemented', 83), ('second', 83), ('summary', 83), ('result', 82), ('score', 82), ('sequence', 82), ('describe', 81), ('recall', 81), ('feature', 81), ('interface', 80), ('important', 80), ('design', 80), ('relations', 79), ('thus', 79), ('learning', 79), ('current', 79), ('full', 79), ('objects', 79), ('see', 78), ('tree', 78), ('overall', 78), ('algorithms', 78), ('automatically', 77), ('coverage', 77), ('co', 77), ('make', 77), ('per', 77), ('means', 76), ('tasks', 76), ('statistical', 76), ('component', 75), ('software', 75), ('various', 75), ('describes', 74), ('better', 74), ('shown', 74), ('provides', 73), ('produced', 73), ('previous', 73), ('project', 73)]\n",
      "3015\n"
     ]
    }
   ],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    with open(filename,'r') as fd:\n",
    "        lines = fd.read()\n",
    "        #transforming data to lower \n",
    "        lines = lines.lower()\n",
    "        lines_list = lines.split()\n",
    "        #some files doesn't contain abstract and it thros error so to eliminate it we are using try cath\n",
    "        try:\n",
    "            lines = lines_list[1:lines_list.index('introduction')-1]\n",
    "            if len(lines)<10:\n",
    "                lines = lines_list[0:250]\n",
    "        except:\n",
    "            lines = lines_list[0:250]\n",
    "        lines = ' '.join([str(elem) for elem in lines]) \n",
    "        tokens = clean_doc(lines)\n",
    "        # update counts\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    \n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# add all docs to vocab\n",
    "\n",
    "for file in files:\n",
    "    #Need to give path of cleaned_acl_arc with /\n",
    "    open_file = r'C:\\Users\\amand\\OneDrive\\Desktop\\text mining project\\corpus\\cleaned_acl_arc/' + file\n",
    "    print(file)\n",
    "    add_doc_to_vocab(open_file, vocab)\n",
    "        \n",
    "\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(200))\n",
    "\n",
    "min_occurance = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert entire data into data ready to feed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab_filename = r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\vocabulary.txt'\n",
    "file = open(vocab_filename, 'r',errors='ignore')\n",
    "text = file.read()\n",
    "vocab = text.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    txt_file = open(filename, 'r',errors='ignore')\n",
    "    # read all text\n",
    "    text = txt_file.read()\n",
    "    text = text.lower()\n",
    "    text_list = text.split()\n",
    "    \n",
    "    try:\n",
    "        text = text_list[1:text_list.index('introduction')-1]\n",
    "        if len(text)<10:\n",
    "                text = text_list[0:250]\n",
    "    except:\n",
    "        text = text_list[0:250]\n",
    "    text = ' '.join([str(elem) for elem in text]) \n",
    "    # close the file\n",
    "    txt_file.close()\n",
    "    return text\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    rtr = []\n",
    "    split_text = doc.split()\n",
    "    for tokens in split_text:\n",
    "        tokens = tokens.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        if len(tokens) > 0:\n",
    "            rtr.append(tokens)\n",
    "    return rtr\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    file = open(filename, 'w')\n",
    "    for line in lines:\n",
    "        data = ' '.join(line)\n",
    "        if len(data) >=1 :\n",
    "            file.write(data)\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def doc_to_line(filename,file, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename+file)\n",
    "    final = []\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    if len(tokens) == 0:\n",
    "        print(file)\n",
    "    # filter by vocab\n",
    "    for i in range (len(tokens)):\n",
    "        t = [w for w in tokens[i] if w in vocab]\n",
    "        final.append(' '.join(t))\n",
    "    final = list(filter(None, final)) \n",
    "    return final\n",
    "         \n",
    " \n",
    "# load vocabulary\n",
    "vocab_filename = r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\vocabulary.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "nlp = []\n",
    "for file in files:\n",
    "    #Need to give path of cleaned_acl_arc with /\n",
    "    nlp.append(doc_to_line(r'C:\\Users\\amand\\OneDrive\\Desktop\\text mining project\\corpus\\cleaned_acl_arc/' ,file,vocab))\n",
    "\n",
    "save_list(nlp, r'C:\\Users\\amand\\OneDrive\\Desktop\\tmp2\\finished.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
