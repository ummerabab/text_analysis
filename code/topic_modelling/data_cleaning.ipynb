{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Gathering data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "#Accessing data from corpus\n",
    "# Replace with the destination of corpus on your local system\n",
    "corpus_root = '/Users/ummerabab-/Desktop/acl_corpus' \n",
    "corpus_files = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
    "\n",
    "# Number of texts in the corpus\n",
    "len (corpus_files.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and read all text files to the dictionary 'files'\n",
    "files = {}\n",
    "\n",
    "#papers = []\n",
    "texts = []\n",
    "\n",
    "for i, c in enumerate(corpus_files.fileids()):\n",
    "    \n",
    "    with open(r'/Users/ummerabab-/Desktop/acl_corpus/' + c, \"r\", encoding = 'unicode_escape') as file:\n",
    "        \n",
    "        files [i] = file.read()\n",
    "        #papers.append(c)\n",
    "        texts.append(files[i])\n",
    "        \n",
    "\n",
    "\n",
    "# Print text file value from dictionary against key[0]\n",
    "print(files[0])\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Serialize the dictionary 'files'\n",
    "filename = 'files_dict'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(files,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Cleaning the data\n",
    "\n",
    "# We can either keep the data in dictionary format or put it into a pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "# intialise data of lists.\n",
    "df_dict = {'Texts': texts}\n",
    " \n",
    "# Create DataFrame\n",
    "corpus_df = pd.DataFrame(df_dict)\n",
    " \n",
    "# Print the output.\n",
    "print(corpus_df)\n",
    "\n",
    "# Serialize the corpus\n",
    "filename = 'corpus_df'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(corpus_df,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "clean_data_array = []\n",
    "\n",
    "for i in texts:\n",
    "\n",
    "    #All characters lower case, remove square brackets, punctuation and numbers\n",
    "    content = i\n",
    "    content = content.lower()\n",
    "    content = re.sub('\\[.*?\\]', '', content)\n",
    "    content = re.sub('[%s]' % re.escape(string.punctuation), '', content)\n",
    "    content = re.sub('\\w*\\d\\w*', '', content)\n",
    "    #content = re.sub('[‘’“”…]', '', content)\n",
    "    content = re.sub('\\n', '', content)\n",
    "    \n",
    "    clean_data_array.append(content)\n",
    "    \n",
    "    \n",
    "# Check data after cleaning\n",
    "print(clean_data_array[0])\n",
    "\n",
    "\n",
    "# Serialize the clean_data_array\n",
    "filename = 'clean_data_array'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(clean_data_array,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2 - Organizing data\n",
    "\n",
    "'Document Term Matrix'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countVectorizer = CountVectorizer( stop_words = 'english')\n",
    "X = countVectorizer.fit_transform(clean_data_array)\n",
    "dtm_df = pd.DataFrame ( X.toarray(), columns = countVectorizer.get_feature_names())\n",
    "\n",
    "print(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the Document Term Matrix\n",
    "filename = 'DTM'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(dtm_df,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/adashofdata/nlp-in-python-tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
